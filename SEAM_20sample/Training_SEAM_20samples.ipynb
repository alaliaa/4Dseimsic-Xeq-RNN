{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from prep_data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from LSTM_Net import LSTM_Net \n",
    "import torch.nn as nn \n",
    "import torch \n",
    "import numpy as np\n",
    "import math \n",
    "import random\n",
    "import gc\n",
    "import time \n",
    "from skimage.transform import resize\n",
    "from util import * \n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import os,sys\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEED for reproducability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '3'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths and parameters\n",
    "data_path = '../LSTM_SEAM_submitted_v2/data'\n",
    "base_path = f\"{data_path}/base_seam.npy\" \n",
    "mon_path = f\"{data_path}/mon_seam.npy\" \n",
    "\n",
    "parm = {\n",
    " 'nt': 2500,\n",
    " 'ng': 501,\n",
    " 'ns': 60,\n",
    " 'dt':  0.0024,\n",
    " 'dg': 0.02500000037252903,\n",
    " 'ds': 0.17499999701976776,\n",
    " 'ot': 0.0,\n",
    " 'og': 0.0,\n",
    " 'os': 1.0}\n",
    " \n",
    "\n",
    "mxoffset = 5 # km \n",
    "\n",
    "\n",
    "# Hyperparameters for training\n",
    "batchsz = 64 \n",
    "num_epochs = 300 \n",
    "LR = 0.002\n",
    "hsz = 100\n",
    "act = 'tanh'\n",
    "n_layer=2\n",
    "bias= False\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "# define the overburden window \n",
    "# start= 100 \n",
    "# last = 650 \n",
    "\n",
    "feature_length=41\n",
    "\n",
    "# # define the overburden window \n",
    "start = 550\n",
    "last =  900 \n",
    "\n",
    "epoch=0\n",
    "\n",
    "hyperpar = f\"_lr{LR}_nlayer{n_layer}_act{act}_bias{bias}_batchsz{batchsz}-featurelength{feature_length}_time_start{start}_end{last}_mxoffset{mxoffset}\"\n",
    "\n",
    "net_path = \"./Network/\"\n",
    "\n",
    "# -----------------------------------------------#\n",
    "# RSF PATH \n",
    "# path = '/home/alaliaa/Time_lapse_ML/ML_Models2/data/'\n",
    "# base = path + 'Base_data.rsf'\n",
    "# mon = path + 'MonNoisy_data.rsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base = np.load(base_path)\n",
    "mon = np.load(mon_path)\n",
    " \n",
    "ns,ng,nt = base.shape\n",
    "print ('ns',ns,'ng',ng,'nt',nt)\n",
    "\n",
    "# base = muting(base,mxoffset,parm)\n",
    "# mon  = muting(mon,mxoffset,parm)\n",
    "\n",
    "# # Take only few shots \n",
    "# base = base[30:50,]\n",
    "# mon = mon[30:50,]\n",
    "# ns,ng,nt_ = base.shape\n",
    "\n",
    "# Take only few shots \n",
    "# base = base[20:40:2,]\n",
    "# mon = mon[20:40:2,]\n",
    "# ns,ng,nt_ = base.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "base = base.reshape((ns*ng,nt))\n",
    "mon = mon.reshape((ns*ng,nt))\n",
    "\n",
    "print('data shapeed to 2d',base.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip=3\n",
    "# base = base[::skip,]\n",
    "# mon = mon[::skip,]\n",
    "# print(f\"data shape after slipping {skip} traces: {base.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_shot(data,par):\n",
    "    vmin, vmax = np.percentile(data, [2,98])\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(data[0*ng:10*ng,:].T, aspect='auto',\n",
    "           vmin=vmin, vmax=vmax,cmap='gray',extent=[par['og'],par['dg']*par['ng']+par['og'],\n",
    "                                                    par['nt']*par['dt']+par['ot'],par['ot']])\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.xlabel('Distance (km)')\n",
    "\n",
    "# This plot 2D concatenated shots \n",
    "Plot_shot(base-mon,parm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pre Processing the data for trainign \n",
    "    - specify the overburden time window \n",
    "    - split the data into training and testing \n",
    "    - scale the data \n",
    "\n",
    "'''\n",
    "\n",
    "def scale_data(data=None):\n",
    "    print('shape of data that will be scaled', data.shape)\n",
    "    assert len(data.shape)==2\n",
    "    ng,nt = data.shape  \n",
    "    \n",
    "    data = data.T\n",
    "    scaler= MinMaxScaler(feature_range=(-1,1))\n",
    "    # scaler= StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    scaled_data = scaled_data.T \n",
    "\n",
    "\n",
    "    return scaled_data,scaler\n",
    "         \n",
    "\n",
    "\n",
    "def min_max_custom(seq,a,b): \n",
    "    minx = seq.min()\n",
    "    maxx = seq.max()\n",
    "    seq = (b-a) * (seq-minx)/(maxx-minx)  + a \n",
    "    return seq, minx,maxx\n",
    "def min_max_fit_custom(seq,a,b,minx,maxx): \n",
    "\n",
    "    seq = (b-a) * (seq-minx)/(maxx-minx)  + a \n",
    "    return seq\n",
    "\n",
    "\n",
    " \n",
    "base = base[:,start:last]\n",
    "base = base[~np.all(base==0 , axis=1)] # this exclude the zero traces \n",
    "mon = mon[:,start:last]   \n",
    "mon = mon[~np.all(mon==0 , axis=1)] # this exclude the zero traces \n",
    "\n",
    "\n",
    "# split data \n",
    "test_size = 0.2\n",
    "base_train,base_test,mon_train,mon_test  =  train_test_split(base,mon,test_size=test_size,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ntrain = base_train.shape[0]\n",
    "Ntest = base_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# scaling\n",
    "# base_train, base_train_scaler = scale_data(base_train) \n",
    "# mon_train , mon_train_scaler  = scale_data(mon_train) \n",
    "# base_test , base_test_scaler  = scale_data(base_test) \n",
    "# mon_test  , mon_test_scaler   = scale_data(mon_test) \n",
    "\n",
    "base_train,min_in,max_in = min_max_custom (base_train,-1,1)\n",
    "mon_train,min_op,max_op = min_max_custom (mon_train,-1,1)\n",
    "base_test = min_max_fit_custom(base_test,-1,1,min_in,max_in)\n",
    "mon_test  = min_max_fit_custom(mon_test,-1,1,min_op,max_op)\n",
    "\n",
    "# move to torch \n",
    "base_train = torch.from_numpy(base_train).double()\n",
    "mon_train  = torch.from_numpy(mon_train).double()\n",
    "base_test  = torch.from_numpy(base_test).double()\n",
    "mon_test   = torch.from_numpy(mon_test).double()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "\n",
    "minmax = {'min_in':min_in, 'max_in':max_in, 'min_op':min_op, 'max_op':max_op}\n",
    "\n",
    "# with open('./mu_std.pck','wb') as f:\n",
    "#     pickle.dump(mu_std,f)\n",
    "\n",
    "with open('./minmax.pck','wb') as f:\n",
    "    pickle.dump(minmax,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot_shot(data,par):\n",
    "    vmin, vmax = np.percentile(data, [2,98])\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.imshow(data[0*ng:10*ng,:].T, aspect='auto',\n",
    "           vmin=vmin, vmax=vmax,cmap='gray',extent=[par['og'],par['dg']*par['ng']+par['og'],\n",
    "                                                    par['nt']*par['dt']+par['ot'],par['ot']])\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.xlabel('Distance (km)')\n",
    "\n",
    "# This plot 2D concatenated shots \n",
    "Plot_shot(base,parm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare input window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window (x,dimension=-1,size=11,step=1):\n",
    "    ''' x has to be pytorch tensor'''\n",
    "    return x.unfold(dimension,size,step)\n",
    "\n",
    "\n",
    "print(base_train.shape)\n",
    "base_train_win =window(base_train,size=feature_length,step=1)\n",
    "mon_train_win =window(mon_train,size=feature_length,step=1)[:,:,feature_length//2]\n",
    "mon_train_win = mon_train_win.view(mon_train_win.shape[0],mon_train_win.shape[1],-1)\n",
    "\n",
    "base_test_win =window(base_test,size=feature_length,step=1)\n",
    "mon_test_win =window(mon_test,size=feature_length,step=1)[:,:,feature_length//2]\n",
    "mon_test_win = mon_test_win.view(mon_test_win.shape[0],mon_test_win.shape[1],-1)\n",
    "\n",
    "\n",
    "print(base_train_win.shape),print(mon_train_win.shape)\n",
    "print(base_test_win.shape),print(mon_test_win.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data to be trained \n",
    "training_data = Dataset(base_train_win,mon_train_win)\n",
    "testing_data = Dataset(base_test_win,mon_test_win)\n",
    "\n",
    "train_loader= DataLoader (dataset=training_data,batch_size=batchsz,shuffle=True)\n",
    "test_loader= DataLoader(dataset=testing_data,batch_size=batchsz,shuffle=True)\n",
    "\n",
    "# to see the batch that are loaded \n",
    "# next(iter(data_loader)) ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up network and \n",
    "\n",
    "model = LSTM_Net(input_size=base_train_win.shape[-1], hidden_layer_size=hsz, \n",
    "                output_size=mon_train_win.shape[-1],\n",
    "                batch_sz=batchsz,num_lstm_layer=n_layer,activation=act,dropout=dropout,bias=bias)\n",
    "# model = LSTM_Net2(1,hsz,1,batchsz)\n",
    "model = model.double()\n",
    "model.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "# optimizer = torch.optim.Adam(model.parameters(),lr=LR,weight_decay=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer,'min',verbose=True,patience=5,factor=0.1)\n",
    "loss_arr=[]\n",
    "loss_arr_test=[]\n",
    "R2_train = []\n",
    "R2_test = []\n",
    "grad_norm = [] \n",
    "\n",
    "print(f\"number of training samples {Ntrain} and validation sample {Ntest}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_test,loader_test in enumerate(test_loader,0):\n",
    "#     inp, label = loader_test\n",
    "\n",
    "#     plt.imshow(label[:,:,-1].numpy().T)\n",
    "#     plt.axis('tight')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_test,loader_test in enumerate(train_loader,0):\n",
    "#     inp, label = loader_test\n",
    "\n",
    "#     plt.imshow(label[:,:,-1].numpy().T)\n",
    "#     plt.axis('tight')\n",
    "#     plt.colorbar()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop \n",
    "\n",
    "start_time = time.time()\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "        epoch_loss = 0\n",
    "        R2_train_running=0\n",
    "        total_norm =0\n",
    "        # loop over batches\n",
    "        for i, loader, in enumerate(train_loader,0):\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = loader\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            if inputs.shape[0]!= batchsz: break\n",
    "            model.h_init() # reset the hidden state for new series  \n",
    "            inputs = inputs.view(inputs.shape[0],inputs.shape[1],-1)\n",
    "            labels = labels.view(labels.shape[0],labels.shape[1],-1)\n",
    "            y_pred = model(inputs)\n",
    "            batch_loss = criterion(y_pred,labels)\n",
    "            R2_train_running += r2_score(target=labels,prediction=y_pred)\n",
    "            epoch_loss +=  batch_loss.item()\n",
    "            batch_loss.backward()\n",
    "            model.hidden_cell[0].detach_()\n",
    "            model.hidden_cell[1].detach_()\n",
    "            # Gradient clipping to avoid exploding problem \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=2,norm_type=2)\n",
    "\n",
    "            optimizer.step()\n",
    "            for p in model.parameters():\n",
    "                param_norm = p.grad.detach().data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "            total_norm += total_norm ** 0.5\n",
    "            \n",
    "        total_norm = total_norm/(i+1)    \n",
    "        epoch_loss = epoch_loss / (i+1) # average by the number batch loops \n",
    "        R2_train_running = R2_train_running / (i+1) # average by the number batch loops \n",
    "  \n",
    "        with   torch.no_grad():\n",
    "            epoch_loss_test =0\n",
    "            R2_test_running=0\n",
    "            for i_test,loader_test in enumerate(test_loader,0):\n",
    "                test_inp, test_label = loader_test\n",
    "                if test_inp.shape[0]!= batchsz: break\n",
    "                model.h_init()\n",
    "                x_test = test_inp.cuda()\n",
    "                y_test = test_label.cuda()  \n",
    "                x_test = x_test.view(x_test.shape[0],x_test.shape[1],-1)\n",
    "                y_test = y_test.view(y_test.shape[0],y_test.shape[1],-1)   \n",
    "                test_pred = model(x_test)\n",
    "                test_loss = criterion(test_pred,y_test)\n",
    "                R2_test_running += r2_score(target = y_test,prediction = test_pred)\n",
    "                epoch_loss_test += test_loss.item()\n",
    "                # R2_test_running += R2\n",
    "        epoch_loss_test = epoch_loss_test/ (i_test+1) # average by the number batch loops \n",
    "        R2_test_running = R2_test_running/  (i_test+1) # average by the number batch loops \n",
    "        scheduler.step(epoch_loss_test)  # this should reduce LR on plateau\n",
    "        grad_norm.append(total_norm)\n",
    "        loss_arr.append(epoch_loss)\n",
    "        loss_arr_test.append(epoch_loss_test)\n",
    "        R2_train.append(R2_train_running)\n",
    "        R2_test.append(R2_test_running)\n",
    "        print(f'epoch: {epoch+1:3}/{num_epochs:3}  Training_loss: {epoch_loss:.5e} Validation_loss: {epoch_loss_test:.5e} \\n R2 training: {R2_train_running:.2} R2 test: {R2_test_running:.2}')\n",
    "\n",
    "        if epoch%5 ==0 : torch.save(model.state_dict(),f'{net_path}LSTM_SEAM_epc{epoch}{hyperpar}.pth')\n",
    "\n",
    "        writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "        writer.add_scalar('Loss/test',epoch_loss_test, epoch)\n",
    "        writer.add_scalar('grad_norm',total_norm, epoch)\n",
    "\n",
    "        \n",
    "\n",
    "end_time = time.time()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training time is  {(end_time-start_time)/60} min \")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{net_path}LSTM_SEAM_epc{epoch}{hyperpar}.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model \n",
    "# old net in './Network/LSTM_Otway.pth'\n",
    "\n",
    "torch.save(model.state_dict(),f'{net_path}LSTM_SEAM_epc{epoch}{hyperpar}.pth')\n",
    "\n",
    "loss_arr = np.array(loss_arr)\n",
    "loss_arr_test = np.array(loss_arr_test) \n",
    "\n",
    "np.save(f'./loss_arr/Training_loss_SEAM{hyperpar}',loss_arr)\n",
    "np.save(f'./loss_arr/Testing_loss_SEAM{hyperpar}',loss_arr_test)\n",
    "# np.save('./loss_arr/Training_R2_Otway',np.array(R2_train))\n",
    "# np.save('./loss_arr/Testing_R2_Otway',np.array(R2_test))\n",
    "\n",
    "plot_history(loss_arr,loss_arr_test,'Fig/loss_SEAM{hyperpar}')\n",
    "\n",
    "\n",
    "# plot_r2(R2_train,R2_test,'Fig/R2_Otway')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(trainingloss,testingloss,netname):\n",
    "\n",
    "    Epc = np.arange(1,trainingloss.shape[0]+1)\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "\n",
    "    ax.plot(trainingloss,color='b',label='Training')\n",
    "    ax.plot(testingloss,color='r',label='Validation')\n",
    "\n",
    "    ax.grid(which='both')\n",
    "    ax.set_xlabel('Epochs', fontsize=16, fontweight='bold')\n",
    "    ax.set_ylabel('Loss ',fontsize=16, fontweight='bold')\n",
    "\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.0e'))\n",
    "\n",
    "    ax.tick_params(axis='both',which='minor',labelsize=16)\n",
    "    plt.xticks(fontsize=16,fontweight='semibold')\n",
    "    plt.yticks(fontsize=16,fontweight='semibold')\n",
    "    ax.legend(prop={'size': 16, 'weight':'bold'})\n",
    "\n",
    "    name=netname+'.png'\n",
    "    fig.savefig(name, bbox_inches='tight')\n",
    "\n",
    "    \n",
    "plot_history(loss_arr,loss_arr_test,'Fig/loss_SEAM{hyperpar}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae92f713e3be4d85c70b44d0641ea6a1c78108a8b7c8271cffc1c7065c80a6de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
