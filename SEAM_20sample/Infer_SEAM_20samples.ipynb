{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TL import preproc_TL \n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from prep_data import Dataset \n",
    "from torch.utils.data import DataLoader\n",
    "from LSTM_Net import LSTM_Net\n",
    "import torch.nn as nn \n",
    "import torch \n",
    "import numpy as np\n",
    "import math \n",
    "import gc\n",
    "import time \n",
    "from skimage.transform import resize\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from util import * \n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fed the seed for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data paths and parameters\n",
    "data_path = '../LSTM_SEAM_submitted_v2/data/'\n",
    "base_path = f\"{data_path}/base_seam.npy\" \n",
    "mon_path = f\"{data_path}/mon_seam.npy\" \n",
    "Mclean_path = f\"{data_path}/mon_seam_c.npy\"\n",
    "\n",
    "parm = {\n",
    " 'nt': 2500,\n",
    " 'ng': 501,\n",
    " 'ns': 60,\n",
    " 'dt':  0.0024,\n",
    " 'dg': 0.02500000037252903,\n",
    " 'ds': 0.17499999701976776,\n",
    " 'ot': 0.0,\n",
    " 'og': 0.0,\n",
    " 'os': 1.0}\n",
    " \n",
    "\n",
    "dt = parm['dt']\n",
    "mxoffset = 5 # km \n",
    "\n",
    "\n",
    "# Hyperparameters for training\n",
    "batchsz = 64 \n",
    "num_epochs = 300 \n",
    "LR = 0.002\n",
    "hsz = 100\n",
    "act = 'tanh'\n",
    "n_layer=2\n",
    "bias= False\n",
    "dropout = 0.0\n",
    "\n",
    "\n",
    "# # define the overburden window \n",
    "start = 550\n",
    "last =  900 \n",
    "\n",
    "# feature_length = 21\n",
    "feature_length = 41\n",
    "\n",
    "epoch=295\n",
    "epoch=299\n",
    "# hyperpar = f\"_lr{LR}_nlayer{n_layer}_act{act}_bias{bias}_batchsz{batchsz}_time_start{start}_end{last}_mxoffset{mxoffset}\"\n",
    "hyperpar = f\"_lr{LR}_nlayer{n_layer}_act{act}_bias{bias}_batchsz{batchsz}-featurelength{feature_length}_time_start{start}_end{last}_mxoffset{mxoffset}\"\n",
    "\n",
    "net_path = \"./Network/\"\n",
    "net = f'{net_path}LSTM_SEAM_epc{epoch}{hyperpar}.pth'\n",
    "\n",
    "\n",
    "last =  1600 \n",
    "\n",
    "batchsz = 1 \n",
    "\n",
    "# -----------------------------------------------#\n",
    "# RSF PATH \n",
    "# path = '/home/alaliaa/Time_lapse_ML/ML_Models2/data/'\n",
    "# base = path + 'Base_data.rsf'\n",
    "# mon = path + 'MonNoisy_data.rsf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ns 60 ng 501 nt 2500\n",
      "data shapeed to 2d (30060, 2500)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base = np.load(base_path)\n",
    "mon = np.load(mon_path)\n",
    "Mclean = np.load(Mclean_path)\n",
    "\n",
    "\n",
    "ns,ng,nt = base.shape\n",
    "print ('ns',ns,'ng',ng,'nt',nt)\n",
    "\n",
    "\n",
    "# Take only few shots \n",
    "# base = base[20:41:4,]\n",
    "# mon = mon[20:41:4,]\n",
    "# Mclean = Mclean[20:41:4,]\n",
    "# ns,ng,nt_ = base.shape\n",
    "\n",
    "base = base.reshape((ns*ng,nt))\n",
    "mon = mon.reshape((ns*ng,nt))\n",
    "Mclean = Mclean.reshape((ns*ng,nt))\n",
    "\n",
    "print('data shapeed to 2d',base.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pre Processing the data for trainign \n",
    "    - specify the infer time window \n",
    "    - scale the data \n",
    "    - Rolling window ??! \n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def scale_data(data=None):\n",
    "    print('shape of data that will be scaled', data.shape)\n",
    "    assert len(data.shape)==2\n",
    "    ng,nt = data.shape  \n",
    "    \n",
    "    data = data.T\n",
    "    scaler= MinMaxScaler(feature_range=(-1,1))\n",
    "    # scaler= StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    scaled_data = scaler.transform(data)\n",
    "    scaled_data = scaled_data.T \n",
    "\n",
    "\n",
    "    return scaled_data,scaler\n",
    "    return scaled_data,scaler\n",
    "\n",
    "\n",
    "def min_max_fit_custom(seq,a,b,minx,maxx): \n",
    "\n",
    "    seq = (b-a) * (seq-minx)/(maxx-minx)  + a \n",
    "    return seq       \n",
    "\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open('./minmax.pck','rb') as f:\n",
    "    minmax = pickle.load(f)\n",
    "\n",
    "# The training part from the trace \n",
    "base = base[:,start:last]\n",
    "mon = mon[:,start:last] \n",
    "Mclean = Mclean[:,start:last] \n",
    "\n",
    "\n",
    "# Scaling \n",
    "# base[~np.all(base==0 , axis=1)], base_scaler = scale_data(base[~np.all(base==0 , axis=1)]) \n",
    "# mon[~np.all(mon==0 , axis=1)] , mon_scaler   = scale_data(mon[~np.all(mon==0 , axis=1)]) \n",
    "base = min_max_fit_custom(base,-1,1,minmax['min_in'],minmax['max_in'])\n",
    "mon  = min_max_fit_custom(mon,-1,1,minmax['min_op'],minmax['max_op'])\n",
    "\n",
    "# Scaling \n",
    "# base, base_scaler = scale_data(base) \n",
    "# mon , mon_scaler  = scale_data(mon) \n",
    "\n",
    "# move to torch \n",
    "base = torch.from_numpy(base).double()\n",
    "mon  = torch.from_numpy(mon).double()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30060, 1050])\n",
      "torch.Size([30060, 1010, 41])\n",
      "torch.Size([30060, 1010, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def window (x,dimension=-1,size=11,step=1):\n",
    "    ''' x has to be pytorch tensor'''\n",
    "    return x.unfold(dimension,size,step)\n",
    "\n",
    "\n",
    "print(base.shape)\n",
    "base_win =window(base,size=feature_length,step=1)\n",
    "mon_win =window(mon,size=feature_length,step=1)[:,:,feature_length//2]\n",
    "mon_win = mon_win.view(mon_win.shape[0],mon_win.shape[1],-1)\n",
    "\n",
    "\n",
    "print(base_win.shape),print(mon_win.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_Net(\n",
       "  (lstm): LSTM(41, 100, num_layers=2, bias=False, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the mdoel \n",
    "\n",
    "model = LSTM_Net(input_size=base_win.shape[-1], hidden_layer_size=hsz, output_size=mon_win.shape[-1],\n",
    "                batch_sz=batchsz,num_lstm_layer=n_layer,activation=act,dropout=0,bias=bias)\n",
    "# model = LSTM_Net2(1,hsz,1,batchsz)\n",
    "model.load_state_dict(torch.load(net))\n",
    "model.cuda().double()\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Main inference ''' \n",
    "\n",
    "Pred = torch.zeros_like(mon_win)\n",
    "count = 0 \n",
    "# with torch.no_grad():\n",
    "#     for i in range (base_win.shape[0]//batchsz): \n",
    "#         model.h_init()\n",
    "#         inp = base_win[count:count+batchsz,]\n",
    "#         zero_traces = (inp.numpy() != 0).argmax(axis = 1)\n",
    "#         Pred_batch = model(inp[~zero_traces[0]].cuda())\n",
    "#         Pred[count:count+batchsz,] = Pred_batch.detach().cpu()\n",
    "#         count += batchsz\n",
    "        \n",
    "        \n",
    "for i in range(base_win.shape[0]):\n",
    "        inp = base_win[i,]\n",
    "        if np.any(inp.numpy() != 0):\n",
    "            Pred_batch = model(inp.reshape((1,inp.shape[0],inp.shape[1])).cuda())\n",
    "            Pred[i:i+1,] = Pred_batch.detach().cpu()\n",
    "        else: \n",
    "            Pred[i:i+1,]= torch.zeros_like(Pred[i:i+1,],dtype=torch.double)\n",
    "        \n",
    "# Pred = Pred.numpy()[:,:,0]\n",
    "# mon = mon.detach().numpy()[:,:,0]\n",
    "# base = base.detach().numpy()[:,:,0]\n",
    "\n",
    "\n",
    "Pred = Pred[:,:,-1].numpy()\n",
    "base = base.numpy()\n",
    "mon = mon.numpy()\n",
    "\n",
    "# Pred[~np.all(mon==0 , axis=1)] = mon_scaler.inverse_transform(Pred[~np.all(mon==0 , axis=1)].T).T    \n",
    "# base[~np.all(base==0 , axis=1)] = base_scaler.inverse_transform(base[~np.all(base==0 , axis=1)].T).T    \n",
    "# mon[~np.all(mon==0 , axis=1)] = mon_scaler.inverse_transform(mon[~np.all(mon==0 , axis=1)].T).T    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_inv_custom(seq,a,b,minx,maxx): \n",
    "    seq =(seq-a) * (maxx-minx) /(b-a) + minx\n",
    "    return seq \n",
    "\n",
    "Pred = min_max_inv_custom(Pred,-1,1,minmax['min_op'],minmax['max_op'])\n",
    "\n",
    "base = min_max_inv_custom(base,-1,1,minmax['min_in'],minmax['max_in'])\n",
    "mon = min_max_inv_custom(mon,-1,1,minmax['min_op'],minmax['max_op'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=base.reshape((ns,ng,last-start))[:,:,feature_length//2:-(feature_length//2)]\n",
    "M=mon.reshape((ns,ng,last-start))[:,:,feature_length//2:-(feature_length//2)]\n",
    "Mc = Mclean.reshape((ns,ng,last-start))[:,:,feature_length//2:-(feature_length//2)]\n",
    "Pr = Pred.reshape((ns,ng,last-start-feature_length+1))\n",
    "\n",
    "\n",
    "\n",
    "# base= B \n",
    "# mon = M\n",
    "# Mclean = Mc\n",
    "# Pred = Pr\n",
    " \n",
    "# base = base[1*ng:2*ng,:]\n",
    "# mon = mon[1*ng:2*ng,:]\n",
    "# Mclean = Mclean[1*ng:2*ng,:]\n",
    "# Pred = Pred[1*ng:2*ng,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "\n",
    "def RMS(a):\n",
    "    N = a.shape[0]\n",
    "    return np.sqrt(1/N * np.sum(a**2))\n",
    "#     return np.sqrt(np.mean(a**2))\n",
    "\n",
    "def NRMS (B,M,ti,tf):\n",
    "    N = B.shape[0]\n",
    "    b = B[:,ti:tf].copy()\n",
    "    m = M[:,ti:tf].copy()\n",
    "    num = RMS(b-m)\n",
    "    den = RMS(b)+RMS(m)\n",
    "    score = 200 * num/den\n",
    "    return score\n",
    "    \n",
    "def phi(b,m):\n",
    "    num = np.sum(b*m)\n",
    "    den = np.sqrt(np.sum(b**2)*np.sum(m**2))\n",
    "    return num/den\n",
    "    \n",
    "def predictability(B,M,ti,tf):\n",
    "    N = B.shape[0]\n",
    "    b = B[:,ti:tf].copy()\n",
    "    m = M[:,ti:tf].copy()\n",
    "    w = b.shape[-1]\n",
    "    tao = 0\n",
    "    num = 0 \n",
    "    den = 0\n",
    "    for tao in range(w//2): \n",
    "        phi_bm = phi(b[tao:tao+w//2],m[tao:tao+w//2])\n",
    "        phi_bb = phi(b[tao:tao+w//2],b[tao:tao+w//2])\n",
    "        phi_mm = phi(m[tao:tao+w//2],m[tao:tao+w//2])\n",
    "        num   += phi_bm * phi_bm\n",
    "        den   += phi_bb * phi_mm\n",
    "    score = 100 * num/den\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ''' Shot differeces plots ''' \n",
    "# import matplotlib\n",
    "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "# # Shot plotting \n",
    "\n",
    "# # matplotlib.rc('image', cmap='gray')    \n",
    "# matplotlib.rc('image', cmap='seismic') \n",
    "# font = {\n",
    "# 'weight' : 'bold',\n",
    "# 'size'   : 12}\n",
    "# matplotlib.rc('font', **font)\n",
    "# matplotlib.rcParams.update({'font.size': 12})\n",
    "# vmax = 4e-6\n",
    "# vmin = -vmax\n",
    "# # vmax = .15\n",
    "# # vmin = -.15\n",
    "# # vmin, vmax = np.percentile((Mclean-base).T,[2,98])\n",
    "\n",
    "# def plot_differences(shot1,shot2,shot3,ncols,savedir,metric=True):\n",
    "\n",
    "#     # Measure metric window\n",
    "#     ti = 0\n",
    "#     tf = 400\n",
    "#     print(f\"metric window is [{start*dt}-{(start+tf)*dt}]\")\n",
    "#     fig, ax = plt.subplots(1,ncols,figsize=(16,5))\n",
    "#     for i in range (ncols):\n",
    "#         if metric:\n",
    "#             nrms = NRMS(shot1[i],shot2[i],ti,tf)\n",
    "#             pred = predictability(shot1[i],shot2[i],ti,tf)\n",
    "#         shot_diff = shot1[i] - shot2[i]\n",
    "#         im = ax[i].imshow(shot_diff[~np.all(shot3[i]==0 , axis=1)].T,\n",
    "#             vmin=vmin,vmax=vmax,extent=[0,2000,last*dt,start*dt])\n",
    "#         ax[i].axis(\"tight\")\n",
    "#         ax[i].set_ylabel(\"Time(s)\",fontsize=16,fontweight='bold')\n",
    "#         ax[i].axes.xaxis.set_visible(False)\n",
    "#         if metric: \n",
    "#             ax[i].text(0.05,1.25,\n",
    "#                 f\"NRMS {nrms:.2f} \\nPred {pred:.2f}\",size=16)\n",
    "\n",
    "#         divider = make_axes_locatable(ax[i])\n",
    "#         cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "#         fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "#     fig.tight_layout(pad=1.0)\n",
    "#     fig.savefig(savedir, bbox_inches='tight')\n",
    "\n",
    "  \n",
    "    \n",
    "    \n",
    "# # plot_differences(M,Pr-Pr.mean(axis=2, keepdims=True),\n",
    "# #                 ncols=5,savedir=f'./Fig/pred_diff{hyperpar}.png')\n",
    "    \n",
    "# plot_differences(M,Pr,Mc,\n",
    "#                 ncols=5,savedir=f'./Fig/pred_diff{hyperpar}.png')\n",
    "\n",
    "# plot_differences(M, B,Mc,\n",
    "#                 ncols=5,savedir=f'./Fig/before_diff{hyperpar}.png')\n",
    "\n",
    "# plot_differences(Mc,B,Mc,\n",
    "#                 ncols=5,savedir=f'./Fig/target_diff{hyperpar}.png')\n",
    "\n",
    "# plot_differences(Pr-Pr.mean(axis=2, keepdims=True),\n",
    "#                  np.zeros_like(Pr), Mc,\n",
    "#                 ncols=5,savedir=f'./Fig/pred_mon{hyperpar}.png',metric=False)\n",
    "\n",
    "# plot_differences(M, np.zeros_like(M),Mc,\n",
    "#                 ncols=5,savedir=f'./Fig/mon{hyperpar}.png',metric=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_diff = M -Pr\n",
    "# pred_diff = M -(Pr-Pr.mean(axis=2, keepdims=True))\n",
    "before_diff = M-B\n",
    "true_diff = Mc- B\n",
    "\n",
    "# Post processing step for unwanted behavior resulted from the scaling\n",
    "# pred_diff[np.sum(np.abs(pred_diff),axis=2)> 0.1] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_diff shots is saved \n",
      "before_diff shots is saved \n",
      "true_diff shots is saved \n"
     ]
    }
   ],
   "source": [
    "path = './output_data/'\n",
    "\n",
    "nt_resample = 7500\n",
    "# ---------  pred difference \n",
    "pred_diff_all = np.zeros((ns,ng,nt))\n",
    "pred_diff_all[:,:,start+(feature_length//2):last-(feature_length//2)] = pred_diff \n",
    "# Resamble to 7500 \n",
    "pred_diff_all= resize(pred_diff_all,(pred_diff_all.shape[0],pred_diff_all.shape[1],nt_resample))    \n",
    "filename = 'pred_diff.rsf@'\n",
    "filename = path+filename\n",
    "pred_diff_all.astype('float32').tofile(filename)\n",
    "print(\"pred_diff shots is saved \")\n",
    "\n",
    "# ---------  pred before diff \n",
    "before_diff_all = np.zeros((ns,ng,nt))\n",
    "before_diff_all[:,:,start+(feature_length//2):last-(feature_length//2)] = before_diff \n",
    "# Resamble to 5000 \n",
    "before_diff_all= resize(before_diff_all,(before_diff_all.shape[0],before_diff_all.shape[1],nt_resample))    \n",
    "filename = 'before_diff.rsf@'\n",
    "filename = path+filename\n",
    "before_diff_all.astype('float32').tofile(filename)\n",
    "print(\"before_diff shots is saved \")\n",
    "\n",
    "\n",
    "# ---------  true before diff \n",
    "true_diff_all = np.zeros((ns,ng,nt))\n",
    "true_diff_all[:,:,start+(feature_length//2):last-(feature_length//2)] = true_diff\n",
    "# Resamble to 5000 \n",
    "true_diff_all= resize(true_diff_all,(true_diff_all.shape[0],true_diff_all.shape[1],nt_resample))    \n",
    "filename = 'true_diff.rsf@'\n",
    "filename = path+filename\n",
    "true_diff_all.astype('float32').tofile(filename)\n",
    "print(\"true_diff shots is saved \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae92f713e3be4d85c70b44d0641ea6a1c78108a8b7c8271cffc1c7065c80a6de"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
